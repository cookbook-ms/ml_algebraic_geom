{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import training_functions as tf\n",
    "from pickle import load\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "def OrderMatrix(weights):\n",
    "    ''' Orders the columns of the weights matrix cyclically.\n",
    "    Arg:\n",
    "        weights: a list of lists of non-negative integers (given as the rows).\n",
    "    Return:\n",
    "        a list of lists of non-negative integers (now we have ordered this cyclically).\n",
    "    '''\n",
    "    # Get column presentation\n",
    "    weights = [[weights[0][i], weights[1][i]] for i in range(len(weights[0]))]\n",
    "\n",
    "    # Order according to top row first\n",
    "    weights.sort(key = lambda x : x[0])\n",
    "\n",
    "    # Separate into sections\n",
    "    zerou = [i for i in weights if i[0] == 0]\n",
    "    zerov = [i for i in weights if i[1] == 0]\n",
    "    nonzero = [i for i in weights if i[0]*i[1] != 0]\n",
    "\n",
    "    # Order each section (zerov is already ordered from the first .sort we have done)\n",
    "    zerou.sort(key = lambda x: x[1])\n",
    "    nonzero.sort(key = lambda x: x[1]/x[0])\n",
    "\n",
    "    # Regroup the sections\n",
    "    weights = zerov + nonzero + zerou\n",
    "\n",
    "    # Return weights in row presentation\n",
    "    return [[x[0] for x in weights], [x[1] for x in weights]]\n",
    "\n",
    "# Total number of arguments\n",
    "# n = len(sys.argv)\n",
    "\n",
    "# TRAIN_SIZE = 600000\n",
    "# TRAIN_SIZE = 1500000\n",
    "TRAIN_SIZE = 1000000\n",
    "TRAIN_SIZE = 500000\n",
    "# Hard-coded definition of the bound\n",
    "bound = 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/bound_{bound}_terminal_augmented.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    terminal = [ast.literal_eval(x) for x in data]\n",
    "\n",
    "with open(f'../data/bound_{bound}_non_terminal_augmented.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    non_terminal = [ast.literal_eval(x) for x in data]  \n",
    "\n",
    "# Make sure that the column in the weight matrices are cyclically ordered in the same way\n",
    "terminal = [OrderMatrix(x) for x in terminal]\n",
    "non_terminal = [OrderMatrix(x) for x in non_terminal]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'../data/bound_{bound}_terminal_more_augmented.txt', 'r') as f:\n",
    "#     data = f.readlines()\n",
    "#     terminal = [ast.literal_eval(x) for x in data]\n",
    "\n",
    "# with open(f'../data/bound_{bound}_non_terminal_more_augmented.txt', 'r') as f:\n",
    "#     data = f.readlines()\n",
    "#     non_terminal = [ast.literal_eval(x) for x in data]  \n",
    "    \n",
    "# # Make sure that the column in the weight matrices are cyclically ordered in the same way\n",
    "# terminal = [OrderMatrix(x) for x in terminal]\n",
    "# non_terminal = [OrderMatrix(x) for x in non_terminal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/bound_{bound}_terminal.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    terminal = [ast.literal_eval(x) for x in data]\n",
    "\n",
    "with open(f'../data/bound_{bound}_non_terminal.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    non_terminal = [ast.literal_eval(x) for x in data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define configurable model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layers = (200,200,200), slope = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sanity checks\n",
    "        if len(layers) == 0:\n",
    "            raise ValueError('Empty network')\n",
    "\n",
    "        # Input layer\n",
    "        self.inp = nn.Linear(20,layers[0])\n",
    "\n",
    "        # Hidden layers\n",
    "        self.hid = nn.ModuleList()\n",
    "        for i in range(len(layers)-1):\n",
    "            self.hid.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        \n",
    "        # Outputlayer: 2 classes, so only one neuron\n",
    "        self.out = nn.Linear(layers[-1],1)\n",
    "\n",
    "        # Leaky ReLu activation function\n",
    "        self.m = nn.LeakyReLU(slope)\n",
    "    \n",
    "    # We need to define how the data goes through the nn\n",
    "    def forward(self,x):\n",
    "        # Make x pass through every layer\n",
    "        x = self.m(self.inp(x))\n",
    "\n",
    "        for l in self.hid:\n",
    "            x = self.m(l(x)) \n",
    "\n",
    "        x = self.out(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Store the training and validation losses\n",
    "accuracy_train = {}\n",
    "accuracy_test = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define train set\n",
    "X_train = terminal[:TRAIN_SIZE] + non_terminal[:TRAIN_SIZE]\n",
    "y_train = [1]*TRAIN_SIZE + [0]*TRAIN_SIZE\n",
    "\n",
    "# Define testing set\n",
    "X_test = terminal[TRAIN_SIZE:] + non_terminal[TRAIN_SIZE:]\n",
    "y_test = [1]*len(terminal[TRAIN_SIZE:]) + [0]*len(non_terminal[TRAIN_SIZE:])\n",
    "\n",
    "# Shuffle\n",
    "perm = np.random.permutation(len(X_train))\n",
    "X_train = [X_train[i] for i in perm]\n",
    "y_train = [y_train[i] for i in perm]\n",
    "\n",
    "# Shuffle\n",
    "perm = np.random.permutation(len(X_test))\n",
    "X_test = [X_test[i] for i in perm]\n",
    "y_test = [y_test[i] for i in perm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmaosheng/miniconda3/lib/python3.11/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.1.3 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Turn into tensors and flatten\n",
    "y_train = torch.Tensor(y_train)\n",
    "X_train = torch.Tensor(X_train)\n",
    "X_train = torch.flatten(X_train,start_dim=1)\n",
    "\n",
    "y_test = torch.Tensor(y_test)\n",
    "X_test = torch.Tensor(X_test)\n",
    "X_test = torch.flatten(X_test,start_dim=1)\n",
    "\n",
    "# Load scaler and transform testing and training\n",
    "scaler = load(open(f'../trained_models/ml_terminality_scaler{TRAIN_SIZE}_dim8_bound{bound}.pkl', 'rb'))\n",
    "# \n",
    "X_train = torch.Tensor(scaler.transform(X_train))\n",
    "X_test = torch.Tensor(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the training accuracy\n",
    "\n",
    "# Final configuration for the model\n",
    "config = {\"layers\": (512, 768, 512),\n",
    "    \"lr\": 0.01,\n",
    "    \"batch_size\": 128,\n",
    "    \"momentum\": 0.99, \n",
    "    \"slope\": 0.01}\n",
    "\n",
    "# Accuracy train\n",
    "# accuracy = tf.TestAccuracy(X_train, y_train, f'ml_terminality_nn{TRAIN_SIZE}_dim8_bound{bound}', net, device=\"cuda:0\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load the model\n",
    "net = Net(layers = config[\"layers\"], slope = config[\"slope\"])\n",
    "net.load_state_dict(torch.load(f'../trained_models/ml_terminality_nn{TRAIN_SIZE}_dim8_bound{bound}.pt', map_location=torch.device('cuda:0')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the training set is:  0.880057\n"
     ]
    }
   ],
   "source": [
    "accuracy = tf.TestAccuracy(X_train, y_train, f'ml_terminality_nn{TRAIN_SIZE}_dim8_bound{bound}', net, device=\"cuda:0\")[0]\n",
    "\n",
    "print('The accuracy on the training set is: ', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the testing set is:  0.83527\n"
     ]
    }
   ],
   "source": [
    "# Get testing accuracy\n",
    "# Initialize the network \n",
    "net = Net(layers = config[\"layers\"], slope = config[\"slope\"])\n",
    "# Accuracy test\n",
    "accuracy, predictions = tf.TestAccuracy(X_test, y_test, f'ml_terminality_nn{TRAIN_SIZE}_dim8_bound{bound}', net, device=\"cuda:0\")\n",
    "print('The accuracy on the testing set is: ', accuracy)\n",
    "\n",
    "# Normalise with respect to both the true values and the predicted values\n",
    "# mat_true = confusion_matrix(y_test, predictions.to('cpu'), normalize='true')\n",
    "# mat_pred = confusion_matrix(y_test, predictions.to('cpu'), normalize='pred')\n",
    "# # Plot and save the confusion matrices\n",
    "# tf.ConfusionMatricesPlotter(mat_true, f'confusion_matrix_terminality_{TRAIN_SIZE}_true_dim8_bound{bound}')\n",
    "# tf.ConfusionMatricesPlotter(mat_pred, f'confusion_matrix_terminality_{TRAIN_SIZE}_pred_dim8_bound{bound}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
